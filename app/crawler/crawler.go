package crawler

import (
	"MedSearch/app/config"
	"MedSearch/app/database"
	"MedSearch/app/database/repository"
	"MedSearch/app/logger"
	"log"
	"strings"
	"time"

	"github.com/PuerkitoBio/goquery"
	"github.com/gocolly/colly/v2"
)

type Crawler struct {
	config *config.CrawlerConfig
	logger *logger.MLogger
}

func NewCrawler(config *config.CrawlerConfig, logger *logger.MLogger) *Crawler {
	newCrawler := Crawler{
		config: config,
		logger: logger,
	}

	database.Connect(config.GetDbURI(), config.GetDbName())
	repository.CreateDrugTextIndex()

	return &newCrawler
}

func (c *Crawler) Start() {
	c.logger.Log("üöÄ Starting crawler...")

	//TODO: Create collector by colly
	collector := colly.NewCollector(
		colly.MaxDepth(10), // Increased depth
	)

	collector.SetRequestTimeout(30 * time.Second)

	collector.Limit(&colly.LimitRule{
		DomainGlob:  "*",
		Parallelism: 4,
		Delay:       1 * time.Second, // Reduced delay
		RandomDelay: 1 * time.Second,
	})

	// Add error handling
	collector.OnError(func(r *colly.Response, err error) {
		c.logger.Log("‚ùå Error visiting %s: %v", r.Request.URL, err)
	})

	//TODO: Set rate limit for collector: OnRequest, OnResponse. OnHTML, OnError
	// Get Info in OnHTML function
	collector.OnRequest(func(r *colly.Request) {
		c.logger.Log("üåê Visiting: %s", r.URL.String())
	})

	//TODO: Set callback for collector
	collector.OnHTML("body", func(e *colly.HTMLElement) {
		url := e.Request.URL.String()
		c.logger.Log("üìÑ Processing: %s", url)

		if strings.Contains(url, "/thuoc/") && strings.Contains(url, ".html") {
			c.logger.Log("üíä Found drug page: %s", url)
			drug := ParseDrug(e)
			start := time.Now()
			err := repository.InsertDrug(&drug)
			elapsed := time.Since(start)
			if err != nil {
				c.logger.Log("‚ùå Failed to insert drug: %v (took %v)", err, elapsed)
			} else {
				c.logger.Log("‚úÖ Successfully inserted drug: %s (took %v)", drug.Name, elapsed)
			}
		} else {
			c.logger.Log("üîó Looking for links on: %s", url)
			linkCount := 0
			e.DOM.Find("a[href]").Each(func(i int, s *goquery.Selection) {
				href, exists := s.Attr("href")
				if exists {
					// Convert relative URLs to absolute URLs
					if strings.HasPrefix(href, "/") {
						href = c.config.GetBaseURL() + href
					} else if strings.HasPrefix(href, "./") {
						href = c.config.GetBaseURL() + href[1:]
					} else if !strings.HasPrefix(href, "http") {
						href = c.config.GetBaseURL() + "/" + href
					}

					// Improved URL filtering - check if it's a drug page
					if strings.Contains(href, "/thuoc/") {
						c.logger.Log("üîó Found drug link: %s", href)
						linkCount++
						e.Request.Visit(href)
					}
				}
			})
			c.logger.Log("üìä Found %d drug links on %s", linkCount, url)
		}
	})

	c.logger.Log("üéØ Starting crawl from: %s", c.config.GetStartURL())
	collector.Visit(c.config.GetStartURL())
	collector.Wait()
	log.Println("üèÅ Crawler finished!")
}
